# -*- coding: utf-8 -*-
"""DepressionRiskAnalysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1thvK435QxFtylJDWQ5fOoQIje3KKVa1T
"""

!pip install praw
!pip install contractions
!pip install tensorboard
!pip install albumentations
!pip install snorkel
!pip install textblob
!pip install --upgrade gensim

import praw
reddit = praw.Reddit(client_id='-XtFJ8ZXlqDkzg', client_secret='jKwJ85Fas9shha_j0a7O4t6gZw3y3A', user_agent='WebScraping')

import pandas as pd
posts = []
subreddit = reddit.subreddit('COVID19_support')
for post in subreddit.hot(limit=2000):
    posts.append([post.title, post.score, post.id, post.num_comments, post.selftext])

posts = pd.DataFrame(posts,columns=['title', 'score', 'id', 'num_comments', 'body'])
pd.set_option('display.max_colwidth', 100000000000)

# delete a single row by index value 0
posts = posts.drop(labels=0, axis=0)
posts[['title','body']]

title = posts['title'].tolist()
body = posts['body'].tolist()

# Pre-processing step
# import necessary libraries
import re, string, unicodedata
import nltk
from nltk import word_tokenize, sent_tokenize
from gensim.parsing.preprocessing import remove_stopwords
from nltk.stem import WordNetLemmatizer

# 1) remove \n from the corpus
def remove_n(sample):
    return re.sub(r"\n", "", sample)

title1 = []
body1 = []
for i in range(0,len(title)) :
    title1.append(remove_n(title[i])) 

for i in range(0,len(body)) :
    body1.append(remove_n(body[i]))

# 2) remove URL from the corpus
def remove_URL(sample):
    return re.sub(r"http\S+", "", sample)

title2 = []
body2 = []
for i in range(0,len(title1)) :
    title2.append(remove_URL(title1[i]))

for i in range(0,len(body1)) :
    body2.append(remove_URL(body1[i]))

# 3) Replace contractions
import contractions
def r_contract(sample) :
    return contractions.fix(sample)

title3 = []
body3 = []
for i in range(0,len(title2)) :
    title3.append(r_contract(title2[i]))

for i in range(0,len(body2)) :
    body3.append(r_contract(body2[i]))

# 4) conversion of uppercase to lowercase 
def lowercase(sample):
    for word in sample:
        return sample.lower() 
title4 = []
body4 = []
for i in range(0,len(title3)) :
    title4.append(lowercase(title3[i]))

for i in range(0,len(body3)) :
    body4.append(lowercase(body3[i]))

# 5) Remove numeric characters
def remove_num(sample):
    return re.sub(r'[\d-]', "", sample)
  
title5 = []
body5 = []
for i in range(0,len(title4)) :
    title5.append(remove_num(str(title4[i])))

for i in range(0,len(body4)):
    body5.append(remove_num(str(body4[i])))

# 6) Remove symbols
def remove_sym(sample):
    sample = re.sub(r"&","", sample)
    sample = re.sub(r",","", sample)
    sample = re.sub(r":","", sample)
    sample = re.sub(r";","", sample)
    sample = re.sub(r"!","", sample)
    sample = re.sub(r"'","", sample)
    sample = re.sub(r'"','', sample)
    sample = re.sub(r"\.","", sample)
    sample = re.sub(r"\?","", sample)
    sample = re.sub(r"\*","", sample)
    sample = re.sub(r'\(','', sample)
    sample = re.sub(r'\)','', sample)
    sample = re.sub(r'\-','', sample)
    sample = re.sub(r'\/','', sample)
    sample = re.sub(r'\@','', sample)
    sample = re.sub(r'\$','', sample)
    sample = re.sub(r'\%','', sample)
    sample = re.sub(r'\|','', sample)
    sample = re.sub(r'\=','', sample)
    sample = re.sub(r'\_','', sample)
    sample = re.sub(r'\^','', sample)
    return re.sub(r'[^\w\s]', '', sample)

title6 = []
body6 = []
for i in range(0,len(title5)) :
    title6.append(remove_sym(str(title5[i])))

for i in range(0,len(body5)):
    body6.append(remove_sym(str(body5[i])))

# 7) Remove the non-ascii characters eg, \u206 and emoticons etc.
def remove_nascii(sample) :
    return unicodedata.normalize('NFKD', sample).encode('ascii', 'ignore').decode('utf-8', 'ignore')

body7 = []
title7 = []
for i in range(0,len(title6)) :
  title7.append(remove_nascii(title6[i]))

for i in range(0,len(body6)) :
  body7.append(remove_nascii(body6[i]))

# 8) Remove the stopwords
def remove_stop(sample) :
    return remove_stopwords(sample)


body8 = []
title8 = []
for i in range(0,len(title7)) :
  title8.append(remove_stop(title7[i]))

for i in range(0,len(body7)) :
  body8.append(remove_stop(body7[i]))

for i in range(0,len(body8)) :
  if body8[i] == 'None' :
      body8[i] = re.sub(r'None','', body8[i], count = 1)

# Merging title and the body for labelling
text = []
for i in range(0,len(title8)):
  text.append(title8[i] + ' ' + body8[i])

df = pd.DataFrame(text, columns = ['Text'])
pd.set_option('display.max_colwidth', 100000000000)

df

# Module 2 (Label this data)
# Use snorkel
from snorkel.labeling import labeling_function
from snorkel.preprocess import preprocessor
from textblob import TextBlob

# Create labelling function based on dictionary approach
from snorkel.labeling import LabelingFunction
SWORDS = r"""\b(anxiety|depression|feeling depressed|doubts|worry|unease|fear|apprehension|gloomy|unhappy|isolated|sad|saddened|died|death|infected|lockdown|isolation|lonely|tension|break down|stress|infected|frustrated|alone|jobless|breathlessness)"""
def sus(sample):
        return 1 if re.search(SWORDS, str(sample)) else -1

USWORDS = r"\b(feel positive|safely vaccinated|normal|thankful|recovery|back to normal|happy|safe|grateful|vaccinated|positive|normalcy|comfortable)"
def unsus(sample):
    return 0 if re.search(USWORDS, str(sample)) else -1

susceptible = LabelingFunction(f"susceptible", f=sus)
unsusceptible = LabelingFunction(f"unsusceptible", f=unsus)

# Create the labeling functions using the textblob sentiment analyzer
@preprocessor(memoize=True)
def textblob_polarity(x):
    scores = TextBlob(x.Text)
    x.polarity = scores.polarity
    return x


# Label susceptible users.
@labeling_function(pre=[textblob_polarity])
def polarity_sus(x):
    return 1 if x.polarity < 0 else -1


# Label unsusceptible users.
@labeling_function(pre=[textblob_polarity])
def polarity_unsus(x):
    return 0 if x.polarity > 0 else -1

# Define the labelling set
df_train = df[0:]

from snorkel.labeling import PandasLFApplier

wlfs = [susceptible,unsusceptible]
blob_lfs = [polarity_sus, polarity_unsus]
lfs = wlfs + blob_lfs 
applier = PandasLFApplier(lfs)
L_train = applier.apply(df_train)

# Analyse the coverage and accuracy
from snorkel.labeling import LFAnalysis
LFAnalysis(L=L_train, lfs=lfs).lf_summary()

print(f"Training set coverage: {100 * LFAnalysis(L_train).label_coverage(): 0.001f}%")

# from snorkel.labeling import LabelModel
from snorkel.labeling import model
from snorkel.labeling.apply import pandas as apply

# Train LabelModel.
label_model = model.LabelModel()
label_model.fit(L_train, n_epochs=100, seed=123, log_freq=20, l2=0.1, lr=0.01)

# Prediction on data
preds_train = label_model.predict(L_train)

x = 0
y = 0
z = 0
for i in range(0,len(preds_train)):
  if preds_train[i] == -1:
    x = x + 1
  elif preds_train[i] == 1:
    y = y + 1
  else:
    z = z + 1

print('The number of un-identifiable users are', x)
print('The number of susceptible users are', y)
print('The number of un-susceptible users are are', z)

# Append classes to the dataframe
df = df.assign(Class = preds_train)

# Module 3 (Creation of a balanced dataset)
# Extract the same number of susceptible and un-susceptible users.
dt1 = df.loc[df['Class'] == 1.0]
dt2 = df.loc[df['Class'] == 0.0]
dt3 = dt2[0:dt1.shape[0]]
dt4 = pd.concat([dt1, dt3])
dt4 = dt4.sample(frac=1).reset_index(drop=True)
x = dt4['Text'].values.tolist()
y = dt4.Class

# Module 4 (Classification) 
# Import the necessary libraries
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
from keras.preprocessing.text import Tokenizer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import LinearSVC
from sklearn.naive_bayes import BernoulliNB
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
pd.set_option('display.max_colwidth', 1000000)

# Convert the sentences into features using TF-IDF technique
tfidfconverter = TfidfVectorizer(max_features=1500, min_df=5, max_df=0.7)
x = tfidfconverter.fit_transform(x).toarray()

# Split the data into training and test sets
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)

# Function for visualizing the decision boundaries
def plot_decision_boundaries(X, y, model_class, **model_params):
    """
    Function to plot the decision boundaries of a classification model.
    This uses just the first two columns of the data for fitting 
    the model as we need to find the predicted value for every point in 
    scatter plot.
    Arguments:
            X: Feature data as a NumPy-type array.
            y: Label data as a NumPy-type array.
            model_class: A Scikit-learn ML estimator class 
            e.g. GaussianNB (imported from sklearn.naive_bayes) or
            LogisticRegression (imported from sklearn.linear_model)
            **model_params: Model parameters to be passed on to the ML estimator
    
    Typical code example:
            plt.figure()
            plt.title("KNN decision boundary with neighbros: 5",fontsize=16)
            plot_decision_boundaries(X_train,y_train,KNeighborsClassifier,n_neighbors=5)
            plt.show()
    """
    try:
        X = np.array(X)
        y = np.array(y).flatten()
    except:
        print("Coercing input data to NumPy arrays failed")
    # Reduces to the first two columns of data
    reduced_data = X[:, :2]
    # Instantiate the model object
    model = model_class(**model_params)
    # Fits the model with the reduced data
    model.fit(reduced_data, y)

    # Step size of the mesh. Decrease to increase the quality of the VQ.
    h = .02     # point in the mesh [x_min, m_max]x[y_min, y_max].    

    # Plot the decision boundary. For that, we will assign a color to each
    x_min, x_max = reduced_data[:, 0].min() - 1, reduced_data[:, 0].max() + 1
    y_min, y_max = reduced_data[:, 1].min() - 1, reduced_data[:, 1].max() + 1
    # Meshgrid creation
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))

    # Obtain labels for each point in mesh using the model.
    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])    

    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),
                         np.arange(y_min, y_max, 0.1))

    # Predictions to obtain the classification results
    Z = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)

    # Plotting
    plt.contourf(xx, yy, Z, alpha=0.4)
    plt.scatter(X[:, 0], X[:, 1], c=y, alpha=0.8)
    plt.xlabel("Feature-1",fontsize=14)
    plt.ylabel("Feature-2",fontsize=14)
    plt.xticks(fontsize=14)
    plt.yticks(fontsize=14)
    return plt

# 1) Model 1(Linear support vector classifier)
model1 = LinearSVC(C=0.2)
model1.fit(x_train, y_train)

y_pred1 = model1.predict(x_test)


print(confusion_matrix(y_test,y_pred1))
print(classification_report(y_test,y_pred1))
print(accuracy_score(y_test, y_pred1))

plt.figure()
plt.title("SVC boundary(Training set)", fontsize=14)
plot_decision_boundaries(x_train, y_train, LinearSVC)
plt.savefig("SVC(train)")
plt.show()

plt.figure()
plt.title("SVC boundary(Test set)", fontsize=14)
plot_decision_boundaries(x_test, y_test, LinearSVC)
plt.savefig("SVC(test)")
plt.show()

# Model 2(Random forest classifier)
model2 = RandomForestClassifier(n_estimators=1000, random_state=0)
model2.fit(x_train, y_train) 

y_pred2 = model2.predict(x_test)

print(confusion_matrix(y_test,y_pred2))
print(classification_report(y_test,y_pred2))
print(accuracy_score(y_test, y_pred2))

plt.figure()
plt.title("RandomForest classifier boundary(Training set)", fontsize=16)
plot_decision_boundaries(x_train, y_train, RandomForestClassifier)
plt.savefig("RFC(train)")
plt.show()

plt.figure()
plt.title("RandonForest classifier boundary(Test set)", fontsize=16)
plot_decision_boundaries(x_test, y_test, RandomForestClassifier)
plt.savefig("RFC(test)")
plt.show()

# Model 3(Logistic regression)
model3 = LogisticRegression(max_iter=100)
model3.fit(x_train, y_train) 

y_pred3 = model3.predict(x_test)

print(confusion_matrix(y_test,y_pred3))
print(classification_report(y_test,y_pred3))
print(accuracy_score(y_test, y_pred3))

plt.figure()
plt.title("Logistic regression boundary(Training set)", fontsize=16)
plot_decision_boundaries(x_train, y_train, LogisticRegression)
plt.savefig("LR(train)")
plt.show()

plt.figure()
plt.title("Logistic regression boundary(Test set)", fontsize=16)
plot_decision_boundaries(x_test, y_test, LogisticRegression)
plt.savefig("LR(test)")
plt.show()

# Model 4(AdaBoost classifier) 
model4 = AdaBoostClassifier(learning_rate=0.8)
model4.fit(x_train, y_train) 

y_pred4 = model4.predict(x_test)

print(confusion_matrix(y_test,y_pred4))
print(classification_report(y_test,y_pred4))
print(accuracy_score(y_test, y_pred4))

plt.figure()
plt.title("AdaBoost classifier boundary(Training set)", fontsize=16)
plot_decision_boundaries(x_train, y_train, AdaBoostClassifier)
plt.savefig("ABC(train)")
plt.show()

plt.figure()
plt.title("AdaBoost classifier boundary(Test set)", fontsize=16)
plot_decision_boundaries(x_test, y_test, AdaBoostClassifier)
plt.savefig("ABC(test)")
plt.show()

# Model 5(Decision tree classifier)
model5 = DecisionTreeClassifier(max_features=1000)
model5.fit(x_train, y_train) 
y_pred5 = model5.predict(x_test)

print(confusion_matrix(y_test,y_pred5))
print(classification_report(y_test,y_pred5))
print(accuracy_score(y_test, y_pred5))

plt.figure()
plt.title("DecisionTree classifier boundary(Training set)", fontsize=16)
plot_decision_boundaries(x_train, y_train, DecisionTreeClassifier)
plt.savefig("DTC(train)")
plt.show()

plt.figure()
plt.title("DecisionTree classifier boundary(Test set)", fontsize=16)
plot_decision_boundaries(x_test, y_test, DecisionTreeClassifier)
plt.savefig("DTC(test)")
plt.show()

# Model 6(Bernoulli naive bayes) 
model6 = BernoulliNB(alpha=0.1)
model6.fit(x_train, y_train) 
y_pred6 = model6.predict(x_test)

print(confusion_matrix(y_test,y_pred6))
print(classification_report(y_test,y_pred6))
print(accuracy_score(y_test, y_pred6))

plt.figure()
plt.title("Naive bayes boundry(Training set)", fontsize=16)
plot_decision_boundaries(x_train, y_train, BernoulliNB)
plt.savefig("NBC(train)")
plt.show()

plt.figure()
plt.title("Naive bayes boundry(Test set)", fontsize=16)
plot_decision_boundaries(x_test, y_test, BernoulliNB)
plt.savefig("NBC(test)")
plt.show()